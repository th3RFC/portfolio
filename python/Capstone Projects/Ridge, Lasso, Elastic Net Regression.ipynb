{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e3ebef",
   "metadata": {},
   "source": [
    "# Introduction: Ridge, Lasso, and Elastic Net Regression with the Ames Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b1bfe",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter Notebook, where we'll be exploring the world of regression analysis using the **Ames Housing Dataset**. In this analysis, we'll not only delve into simple linear regression but also extend our exploration to multiple linear regression and regularized regression techniques like  <span style=\"color:red\">**Ridge**</span>,  <span style=\"color:red\">**Lasso**</span>, and  <span style=\"color:red\">**Elastic Net**</span>. By employing these techniques, we aim to predict a continuous target variable, the house price, based on multiple input features.\n",
    "\n",
    "### Objective:\n",
    "Our primary goal is to understand the relationships between various features of a house, such as its size, overall quality, exterior quality, and others, and its sale price. By constructing a multiple linear regression model, we aim to predict house prices based on these selected features and assess the accuracy and performance of our model using various metrics.\n",
    "\n",
    "$$\\Large \\displaystyle \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2}+ ...++ \\hat{\\beta}_p x_{ip}$$\n",
    "\n",
    " - $\\hat{y}_i$ is the predicted value of the dependent variable (house price) for the $i^{th}$ observation.\n",
    " - $\\hat{\\beta}_0$ is the y-intercept of the regression line.\n",
    " - $\\hat{\\beta}_j$ represents the coefficient of the $j^{th}$ feature.\n",
    " - $x_ij$ is the value of the the $j^{th}$ feature for the $i^{th}$ observation.\n",
    " - $p$ is the total number of features used in the model.\n",
    "\n",
    "\n",
    "### Dataset Overview:\n",
    "The Ames Housing Dataset provides a comprehensive snapshot of the housing market in Ames, Iowa. It contains detailed information about various attributes of houses, from their physical characteristics to sale details. \n",
    "\n",
    "### Structure of this Notebook:\n",
    "1. [Installing and Importing Necessary Libraries](#ch1)\n",
    "2. [Loading the Ames Housing Dataset](#ch2)\n",
    "3. [Data Processing](#ch3)\n",
    "4. [Splitting the Data](#ch4)\n",
    "5. [Training the Models](#ch5)\n",
    "6. [Overview of Model Training Results](#ch6)\n",
    "7. [Making Predictions](#ch7)\n",
    "8. [Evaluating the Model](#ch8)\n",
    "9. [Conclusion](#ch9)\n",
    "\n",
    "By the end of this notebook, you'll have a comprehensive understanding of how to build, train, and evaluate various regression models, including Linear Regression, Ridge, Lasso, and Elastic Net, using Python and scikit-learn. Moreover, you'll be familiar with the power of GridSearchCV in model optimization. Let's embark on this analytical journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af51f69",
   "metadata": {},
   "source": [
    "## 1. Installing and Importing Necessary Libraries <a id='ch1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5edb1a",
   "metadata": {},
   "source": [
    "Before starting our analysis, we need to import the necessary Python libraries that will be used throughout this notebook:\n",
    "\n",
    "- **`pandas`**: A foundational library for data manipulation and analysis. It provides data structures for efficiently storing large datasets and tools for reshaping, aggregating, and merging data.\n",
    "\n",
    "- **`sklearn.datasets`**: From scikit-learn, this module allows us to fetch datasets, including the Ames Housing dataset, providing a convenient way to load data for our analysis.\n",
    "\n",
    "- **`sklearn.model_selection`**: This module offers various utilities for model selection, including `train_test_split` for partitioning our data, `GridSearchCV` for exhaustive search over specified parameter values, and `cross_val_score` for evaluating metric scores by cross-validation.\n",
    "\n",
    "- **`sklearn.linear_model`**: Houses various linear models. In this notebook, we'll be using `LinearRegression` for basic regression, and `Ridge`, `Lasso`, and `ElasticNet` for regularized regression techniques.\n",
    "\n",
    "- **`sklearn.metrics`**: Provides functions for model evaluation, including `mean_squared_error` to measure the average squared difference between actual and predicted values, `r2_score` to compute the coefficient of determination indicating the model's explanatory power, and `make_scorer` to create custom scoring functions for model validation processes.\n",
    "\n",
    "- **`sklearn.impute`**: Contains methods for imputation, which is the process of replacing missing data with substituted values. Here, we'll use `SimpleImputer` to handle missing values in our dataset.\n",
    "\n",
    "- **`sklearn.preprocessing`**: Offers common utility functions and transformer classes to change raw feature vectors into a representation more suitable for downstream estimators. We'll use `StandardScaler` to standardize features, `PolynomialFeatures` to generate polynomial and interaction features, and `OneHotEncoder` to convert categorical variables into a format that can be provided to machine learning algorithms to improve predictions.\n",
    "\n",
    "- **`sklearn.pipeline`**: Provides utilities to build a composite estimator, streamlining many of the routine processes. We'll use `Pipeline` to assemble several steps that can be cross-validated together.\n",
    "\n",
    "- **`sklearn.compose`**: This module provides utilities to work with heterogeneous data and to integrate transformers into a pipeline. We'll use `ColumnTransformer` to apply different preprocessing steps to different subsets of the features.\n",
    "\n",
    "By importing these libraries upfront, we ensure a smooth workflow, allowing us to focus on the core analysis without interruptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36e48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c287da",
   "metadata": {},
   "source": [
    "## 2. Loading the Amex Housing Dataset <a id='ch2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6614a9",
   "metadata": {},
   "source": [
    "The **Ames Housing Dataset** is a comprehensive dataset that provides detailed information about individual residential homes in Ames, Iowa. It contains **79 explanatory variables** describing various aspects of the houses, such as their physical characteristics, location, and sale details. Given the richness of this dataset, it's a popular choice for regression analysis in machine learning.\n",
    "\n",
    "For this analysis, we'll utilize 10 of the available features in the dataset to predict the sale price of the houses:\n",
    "\n",
    "- `GrLivArea`: This represents the above-ground living area in square feet. It's intuitive that the size of the living area would be a significant predictor of house price.\n",
    "\n",
    "- `OverallQual`: This is an overall material and finish quality rating, ranging from 1 (very poor) to 10 (very excellent). Houses with higher quality materials and finishes generally sell for higher prices.\n",
    "\n",
    "- `YearBuilt`: The year the house was originally constructed. Newer houses might fetch higher prices due to modern design, better insulation, newer materials, etc.\n",
    "\n",
    "- `TotalBsmtSF`: Total square feet of the basement area. A larger basement can add significant value to a house, especially if it's finished.\n",
    "\n",
    "- `GarageCars`: Size of the garage in car capacity. A larger garage can add value, especially in areas where parking is at a premium or where winters are harsh.\n",
    "\n",
    "- `GarageArea`: Size of the garage in square feet. A larger garage can add value to a property.\n",
    "\n",
    "- `1stFlrSF`: First-floor square feet. The size of the first floor can be a significant factor in house pricing.\n",
    "\n",
    "- `FullBath`: Number of full bathrooms above grade. Bathrooms can significantly influence a home's value.\n",
    "\n",
    "- `CentralAir`: Central air conditioning can be a significant factor in home pricing, especially in areas with hot summers.\n",
    "\n",
    "- `ExterQual`: Evaluates the quality of the material on the exterior. High-quality materials can enhance the curb appeal and durability of a home, thus affecting its price.\n",
    "\n",
    "These features capture various aspects of a house, including its size, quality, age, and amenities. Including a mix of continuous (like `GrLivArea` and `TotalBsmtSF`) and categorical (like `OverallQual`) features can provide a comprehensive view of the house's characteristics.\n",
    "\n",
    "To load the dataset, we use the `fetch_openml` function from scikit-learn. This function fetches datasets from the **OpenML repository**, making it easy to access a wide range of datasets for machine learning and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56575a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Ames Housing Dataset and load it as a pandas DataFrame\n",
    "housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "\n",
    "# Define the entire dataset as X\n",
    "X = housing.data\n",
    "\n",
    "# Select the desired independent variables (features) \n",
    "selected_features = [\n",
    "    'GrLivArea', 'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', \n",
    "    'GarageArea', '1stFlrSF', 'FullBath', 'CentralAir', 'ExterQual'\n",
    "]\n",
    "\n",
    "# Extract these features from the dataset\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Set the target variable as the sale price of the houses\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b3658",
   "metadata": {},
   "source": [
    "## 3. Splitting the Data: <a id='ch3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5c87a",
   "metadata": {},
   "source": [
    "Before training our linear regression model, it's essential to split the dataset into two parts: a <span style=\"color:red\">**training set**</span> and a <span style=\"color:red\">**testing set**</span>. This allows us to train our model on one subset of the data and then test its performance on a separate, unseen subset. This approach helps evaluate how well our model is likely to perform on new, unseen data.\n",
    "\n",
    "In this notebook, we're using the `train_test_split` function from scikit-learn to achieve this split:\n",
    "\n",
    "- `X_train`, `y_train`: These are the features and target variable for the training set, respectively. The model will learn from this data.\n",
    "\n",
    "- `X_test`, `y_test`: These are the features and target variable for the testing set, respectively. We'll use this data to evaluate the model's performance.\n",
    "\n",
    "We've reserved 20% of the data for testing (`test_size=0.2`). The random_state parameter is set to 1337, ensuring that the data split is reproducible. This means that every time we run this code, we'll get the same train/test split, which is useful for consistent results and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dad961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into training and testing sets, with 20% of the data reserved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3efed4",
   "metadata": {},
   "source": [
    "## 4. Data Processing <a id='ch4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a8808",
   "metadata": {},
   "source": [
    "The `SettingWithCopyWarning` is a common warning in pandas and arises when you try to modify a subset of a DataFrame, which might be a view rather than a copy. This can lead to unexpected behavior because changes to the subset might not reflect in the original DataFrame or vice versa.\n",
    "\n",
    "To fix this, we explicitly create a copy of the DataFrame before making modifications. This ensures that we're working with an independent copy and not a view of the original data.\n",
    "\n",
    "By using `X_clean = X.copy()`, we create an explicit copy of the data, and then all modifications are made to this copy, avoiding the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data to avoid SettingWithCopyWarning\n",
    "X_clean = X_selected.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b337904",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad6fc5",
   "metadata": {},
   "source": [
    "Data preprocessing is a crucial step in the machine learning pipeline. It involves preparing the raw data to make it suitable for model training. For the **Ames Housing Dataset**, we'll focus on several preprocessing tasks:\n",
    "\n",
    "1. <span style=\"color:red\">**Identifying Data Types**</span>: The first step is to categorize each feature based on its data type. Features can be broadly classified into:\n",
    " - **Numerical Features**: These are quantifiable variables, representing a measurable quantity.\n",
    " - **Categorical Features**: These represent distinct categories or labels without any inherent order or priority.\n",
    "\n",
    "\n",
    "2. <span style=\"color:red\">**Handling Missing Values**</span>: Real-world datasets frequently contain missing values, which can hinder the performance of machine learning algorithms. Instead of discarding rows with missing values, we'll address this by imputing them:\n",
    " - **Numerical Columns**: Missing values will be replaced with the column's mean.\n",
    " - **Categorical Columns**: Missing values will be substituted with the most frequent category in the column. This approach ensures we retain valuable data that would otherwise be lost.\n",
    " \n",
    " \n",
    "3. <span style=\"color:red\">**Feature Engineering**</span>: This step involves creating new features or modifying existing ones to enhance the model's predictive power. For our dataset:\n",
    " - **Standardization**: We'll employ the StandardScaler to standardize numerical features, ensuring each has a mean of 0 and a standard deviation of 1. This transformation ensures that each feature contributes equally to the model's performance and aids in the optimization algorithm's convergence during training.\n",
    " - **Polynomial Features**: To capture potential non-linear relationships between the features and the target variable, we'll introduce polynomial features. Specifically, we'll generate quadratic features (degree 2) using the PolynomialFeatures class. This introduces new features derived from the squares of the original ones, potentially helping our model recognize parabolic relationships.\n",
    "\n",
    "\n",
    "4. <span style=\"color:red\">**Constructing the Preprocessing Pipeline**</span>: To streamline the preprocessing steps, we use a `ColumnTransformer`. This allows us to apply different preprocessing steps to different subsets of the features.\n",
    "\n",
    "\n",
    "5. <span style=\"color:red\">**Creating the Final Pipelines for Multiple Models**</span>: To facilitate a systematic comparison of different regression models, we'll set up individual pipelines for each of them. This approach ensures that each model benefits from the same preprocessing steps, allowing for a fair comparison. Here's a breakdown:\n",
    " - **Models List**: We've chosen a set of regression models for our analysis: Linear Regression, Ridge, Lasso, and ElasticNet. Each of these models has its strengths and can provide unique insights into the data.\n",
    " - **Pipelines Dictionary**: For each model, we create a dedicated pipeline that integrates our preprocessing steps with the model. This results in a dictionary of pipelines, one for each model.\n",
    "\n",
    "By meticulously preprocessing our data, we lay a robust foundation for the subsequent modeling phase, ensuring our algorithms operate on clean and well-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2b926",
   "metadata": {},
   "source": [
    "### 4.1 - Identifying Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81d7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "num_cols = X_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X_clean.select_dtypes(exclude=['float64', 'int64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafe4fb",
   "metadata": {},
   "source": [
    "### 4.2 - Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24754d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imputer for numerical columns (using mean as the default strategy)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Define imputer for categorical columns (using the most frequent value as the strategy)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ad181",
   "metadata": {},
   "source": [
    "### 4.3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ed92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard scaler for numerical columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define polynomial feature creator (degree 2 for quadratic features)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51512f58",
   "metadata": {},
   "source": [
    "### 4.4 - Constructing the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe6095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing pipeline to handle both numerical and categorical columns.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([('imputer', num_imputer), ('scaler', scaler), ('poly', poly)]), num_cols),\n",
    "        ('cat', Pipeline([('imputer', cat_imputer), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f7adc",
   "metadata": {},
   "source": [
    "### 4.5 - Creating the Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f991511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to loop through\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet()\n",
    "}\n",
    "\n",
    "pipelines = {}\n",
    "\n",
    "# Create a pipeline for each model\n",
    "for model_name, model in models.items():\n",
    "    pipelines[model_name] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f24f31",
   "metadata": {},
   "source": [
    "## 5. Training the Models <a id='ch5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a705f",
   "metadata": {},
   "source": [
    "After preprocessing our data, the next crucial step is to train our machine learning models. In this analysis, we're not just limiting ourselves to a single model. Instead, we're exploring multiple regression models from scikit-learn, namely `LinearRegression`, `Ridge`, `Lasso`, and `ElasticNet`.\n",
    "\n",
    "The process of training these models involves a few steps:\n",
    "\n",
    "- <span style=\"color:red\">**Defining Hyperparameters Grid**</span>: For each model, we define a set of hyperparameters that we want to tune. For instance, for the `Ridge` and `Lasso` models, we're tuning the `alpha` parameter, which controls the strength of the regularization. For ElasticNet, we're also tuning the `l1_ratio`, which determines the mix of **L1** and **L2 regularization**.\n",
    "\n",
    "- <span style=\"color:red\">**Grid Search with Cross-Validation**</span>: We utilize `GridSearchCV` to perform an exhaustive search over the specified hyperparameter values for each model. This method not only trains the models but also performs cross-validation to determine which hyperparameter combinations yield the best performance.\n",
    "\n",
    "- <span style=\"color:red\">**Storing the Best Models**</span>: After the grid search, we store the best-performing model (with the best hyperparameters) and its cross-validation score for each model type.\n",
    "\n",
    "The code for this process is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f7d219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters grid for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'Ridge': {'regressor__alpha': [0.01, 0.1, 1, 10, 100]},\n",
    "    'Lasso': {'regressor__alpha': [0.01, 0.1, 1, 10, 100]},\n",
    "    'ElasticNet': {\n",
    "        'regressor__alpha': [0.01, 0.1, 1, 10, 100],\n",
    "        'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define multiple scoring metrics\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models and their scores\n",
    "best_models = {}\n",
    "best_r2_scores = {}\n",
    "best_mse_scores = {}\n",
    "\n",
    "# Perform grid search with cross-validation for each model\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, scoring=scoring, refit='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model and its scores\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    best_r2_scores[model_name] = grid_search.cv_results_['mean_test_r2'][grid_search.best_index_]\n",
    "    best_mse_scores[model_name] = grid_search.cv_results_['mean_test_neg_mean_squared_error'][grid_search.best_index_]\n",
    "    \n",
    "    # Convert the cv_results_ to a DataFrame\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96119624",
   "metadata": {},
   "source": [
    "By the end of this step, we'll have trained models for each type, optimized for their best hyperparameters, and ready for evaluation on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed747754",
   "metadata": {},
   "source": [
    "### Code Breakdown: <span style=\"color:red\">GridSearchCV</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e0c0d",
   "metadata": {},
   "source": [
    "`GridSearchCV` is a function provided by scikit-learn that performs an exhaustive search over a specified parameter grid. It trains a model for every combination of hyperparameters in the grid and uses cross-validation to evaluate the performance of each combination. This helps in finding the best hyperparameters for a given model.\n",
    "\n",
    "Here's a breakdown of the parameters and methods used:\n",
    "\n",
    "1. <span style=\"color:red\">**Initializating Parameters**</span>\n",
    " - `pipeline`: This is the model pipeline that will be trained and evaluated. A pipeline typically consists of a sequence of data processing steps followed by a machine learning model. In this case, the pipeline includes preprocessing steps (like imputation, scaling, etc.) and a regression model (like `LinearRegression`, `Ridge`, etc.).\n",
    "\n",
    " - `param_grids[model_name]`: This is the grid of hyperparameters over which the search will be performed. It's a dictionary where keys are hyperparameter names (with the appropriate prefix if they're part of a pipeline) and values are lists of values to try. For example, for the `Ridge` model, it might look like `{'regressor__alpha': [0.01, 0.1, 1, 10, 100]}`, indicating that the `alpha` hyperparameter of the Ridge regressor should be tuned over those values.\n",
    "\n",
    " - `cv=5`: This specifies the number of cross-validation folds. The training data is split into 5 parts (or \"folds\"). The model is trained 5 times, each time using 4 of the folds for training and the remaining fold for validation. This helps in getting a more robust estimate of the model's performance.\n",
    "\n",
    "- `scoring`: This is a dictionary specifying multiple metrics to evaluate the performance of the model during cross-validation. In this case, we're using both the negative mean squared error and the \\$R^2\\$ score.\n",
    "\n",
    "- `refit='r2'`: This parameter indicates which metric should be used to determine the best hyperparameters. Here, we're using the  \\$R^2\\$ score.\n",
    "\n",
    " - `n_jobs=-1`: This parameter specifies how many processors should be used to train and evaluate the models in parallel. A value of `-1` means that all available processors on the machine will be used, speeding up the grid search process.\n",
    "\n",
    "\n",
    "1. <span style=\"color:red\">**Methods and Attributes**</span>\n",
    " - `grid_search.fit(X_train, y_train)`: Starts the grid search, training the model for every combination of hyperparameters using the training data.\n",
    " - `best_models[model_name] = grid_search.best_estimator_`: Extracts and stores the best model for the current model type.\n",
    " - `best_r2_scores[model_name]` and `best_mse_scores[model_name]`: Extract and store the best cross-validation scores achieved by the best model for both R^2 and negative mean squared error metrics.\n",
    " - `results_df = pd.DataFrame(grid_search.cv_results_)`: Converts the detailed results of the grid search into a pandas DataFrame.\n",
    "\n",
    "In summary, the line of code initializes a grid search with cross-validation for the given model pipeline and hyperparameter grid. Once the `fit` method is called on this `grid_search` object, it will train the model for every combination of hyperparameters in the grid, evaluate their performance using 5-fold cross-validation, and store the best model and its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2905056",
   "metadata": {},
   "source": [
    "## 6. Overview of Model Training Results <a id='ch6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcd903",
   "metadata": {},
   "source": [
    "After performing an exhaustive grid search with cross-validation for each regression model, we've compiled the results into a comprehensive table. This table provides insights into the performance of each model-hyperparameter combination, allowing us to compare and identify the best-performing model.\n",
    "\n",
    "Here's a breakdown of the metrics presented in the table:\n",
    "\n",
    "- `mean_fit_time`: Represents the average time (in seconds) taken to fit the model to the training data for each cross-validation fold. This metric gives an idea of the computational efficiency of the model.\n",
    "\n",
    "- `std_fit_time`: Indicates the standard deviation of the fit times across the cross-validation folds. A higher value might suggest variability in the training times, possibly due to differences in the data folds or other factors.\n",
    "\n",
    "- `mean_score_time`: Represents the average time (in seconds) taken to score the model on the validation set for each cross-validation fold. This metric provides insights into how quickly the model can make predictions.\n",
    "\n",
    "- `std_score_time`: Indicates the standard deviation of the score times across the cross-validation folds. Like `std_fit_time`, a higher value might suggest variability in the scoring times.\n",
    "\n",
    "- `param_regressor__alpha` (and other `param_` columns): These columns show the specific hyperparameters used for the model in that row. For instance, `param_regressor__alpha` displays the regularization strength used for Ridge, Lasso, and ElasticNet models.\n",
    "\n",
    "- `split0_test_score` to `split4_test_score`: These columns represent the performance of the model on each of the five cross-validation folds. They provide insights into the consistency of the model's performance across different subsets of the data.\n",
    "\n",
    "- `mean_test_score`: Represents the average performance of the model across all cross-validation folds. This is a crucial metric as it gives an overall idea of how well the model is expected to perform on unseen data.\n",
    "\n",
    "- `std_test_score`: Indicates the standard deviation of the test scores across the cross-validation folds. A smaller value suggests that the model's performance is consistent across different data subsets.\n",
    "\n",
    "- `rank_test_score`: Provides a ranking of the models based on their `mean_test_score`, with 1 being the best.\n",
    "\n",
    "By analyzing these metrics, we can gain a comprehensive understanding of each model's performance, efficiency, and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730dc3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__alpha</th>\n",
       "      <th>param_regressor__l1_ratio</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_r2</th>\n",
       "      <th>split1_test_r2</th>\n",
       "      <th>split2_test_r2</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_r2</th>\n",
       "      <th>rank_test_r2</th>\n",
       "      <th>split0_test_neg_mean_squared_error</th>\n",
       "      <th>split1_test_neg_mean_squared_error</th>\n",
       "      <th>split2_test_neg_mean_squared_error</th>\n",
       "      <th>split3_test_neg_mean_squared_error</th>\n",
       "      <th>split4_test_neg_mean_squared_error</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>std_test_neg_mean_squared_error</th>\n",
       "      <th>rank_test_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111555</td>\n",
       "      <td>0.043172</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'regressor__alpha': 0.1, 'regressor__l1_ratio...</td>\n",
       "      <td>0.864866</td>\n",
       "      <td>-0.125797</td>\n",
       "      <td>0.848594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393954</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.367231e+08</td>\n",
       "      <td>-7.065881e+09</td>\n",
       "      <td>-8.898450e+08</td>\n",
       "      <td>-8.919173e+08</td>\n",
       "      <td>-1.157572e+09</td>\n",
       "      <td>-2.188388e+09</td>\n",
       "      <td>2.440747e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.149420</td>\n",
       "      <td>0.082181</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'regressor__alpha': 0.1, 'regressor__l1_ratio...</td>\n",
       "      <td>0.863165</td>\n",
       "      <td>-0.131350</td>\n",
       "      <td>0.851002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395582</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.485186e+08</td>\n",
       "      <td>-7.100732e+09</td>\n",
       "      <td>-8.756891e+08</td>\n",
       "      <td>-9.310711e+08</td>\n",
       "      <td>-1.160791e+09</td>\n",
       "      <td>-2.203360e+09</td>\n",
       "      <td>2.450602e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.096889</td>\n",
       "      <td>0.041137</td>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'regressor__alpha': 0.01, 'regressor__l1_rati...</td>\n",
       "      <td>0.862856</td>\n",
       "      <td>-0.134728</td>\n",
       "      <td>0.851108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396857</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.506590e+08</td>\n",
       "      <td>-7.121933e+09</td>\n",
       "      <td>-8.750661e+08</td>\n",
       "      <td>-9.334978e+08</td>\n",
       "      <td>-1.162220e+09</td>\n",
       "      <td>-2.208675e+09</td>\n",
       "      <td>2.458551e+09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133434</td>\n",
       "      <td>0.106109</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'regressor__alpha': 0.01, 'regressor__l1_rati...</td>\n",
       "      <td>0.861127</td>\n",
       "      <td>-0.153743</td>\n",
       "      <td>0.851586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404051</td>\n",
       "      <td>4</td>\n",
       "      <td>-9.626413e+08</td>\n",
       "      <td>-7.241278e+09</td>\n",
       "      <td>-8.722576e+08</td>\n",
       "      <td>-9.456730e+08</td>\n",
       "      <td>-1.169978e+09</td>\n",
       "      <td>-2.238366e+09</td>\n",
       "      <td>2.503412e+09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147301</td>\n",
       "      <td>0.092709</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'regressor__alpha': 0.1, 'regressor__l1_ratio...</td>\n",
       "      <td>0.863109</td>\n",
       "      <td>-0.171717</td>\n",
       "      <td>0.846866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411898</td>\n",
       "      <td>5</td>\n",
       "      <td>-9.489024e+08</td>\n",
       "      <td>-7.354088e+09</td>\n",
       "      <td>-9.000006e+08</td>\n",
       "      <td>-8.809041e+08</td>\n",
       "      <td>-1.178482e+09</td>\n",
       "      <td>-2.252475e+09</td>\n",
       "      <td>2.553022e+09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4       0.111555      0.043172         0.010418        0.005000   \n",
       "5       0.149420      0.082181         0.009667        0.001973   \n",
       "0       0.096889      0.041137         0.021173        0.016388   \n",
       "1       0.133434      0.106109         0.010348        0.001218   \n",
       "3       0.147301      0.092709         0.008855        0.001076   \n",
       "\n",
       "  param_regressor__alpha param_regressor__l1_ratio  \\\n",
       "4                    0.1                       0.5   \n",
       "5                    0.1                       0.9   \n",
       "0                   0.01                       0.1   \n",
       "1                   0.01                       0.5   \n",
       "3                    0.1                       0.1   \n",
       "\n",
       "                                              params  split0_test_r2  \\\n",
       "4  {'regressor__alpha': 0.1, 'regressor__l1_ratio...        0.864866   \n",
       "5  {'regressor__alpha': 0.1, 'regressor__l1_ratio...        0.863165   \n",
       "0  {'regressor__alpha': 0.01, 'regressor__l1_rati...        0.862856   \n",
       "1  {'regressor__alpha': 0.01, 'regressor__l1_rati...        0.861127   \n",
       "3  {'regressor__alpha': 0.1, 'regressor__l1_ratio...        0.863109   \n",
       "\n",
       "   split1_test_r2  split2_test_r2  ...  std_test_r2  rank_test_r2  \\\n",
       "4       -0.125797        0.848594  ...     0.393954             1   \n",
       "5       -0.131350        0.851002  ...     0.395582             2   \n",
       "0       -0.134728        0.851108  ...     0.396857             3   \n",
       "1       -0.153743        0.851586  ...     0.404051             4   \n",
       "3       -0.171717        0.846866  ...     0.411898             5   \n",
       "\n",
       "   split0_test_neg_mean_squared_error  split1_test_neg_mean_squared_error  \\\n",
       "4                       -9.367231e+08                       -7.065881e+09   \n",
       "5                       -9.485186e+08                       -7.100732e+09   \n",
       "0                       -9.506590e+08                       -7.121933e+09   \n",
       "1                       -9.626413e+08                       -7.241278e+09   \n",
       "3                       -9.489024e+08                       -7.354088e+09   \n",
       "\n",
       "   split2_test_neg_mean_squared_error  split3_test_neg_mean_squared_error  \\\n",
       "4                       -8.898450e+08                       -8.919173e+08   \n",
       "5                       -8.756891e+08                       -9.310711e+08   \n",
       "0                       -8.750661e+08                       -9.334978e+08   \n",
       "1                       -8.722576e+08                       -9.456730e+08   \n",
       "3                       -9.000006e+08                       -8.809041e+08   \n",
       "\n",
       "   split4_test_neg_mean_squared_error  mean_test_neg_mean_squared_error  \\\n",
       "4                       -1.157572e+09                     -2.188388e+09   \n",
       "5                       -1.160791e+09                     -2.203360e+09   \n",
       "0                       -1.162220e+09                     -2.208675e+09   \n",
       "1                       -1.169978e+09                     -2.238366e+09   \n",
       "3                       -1.178482e+09                     -2.252475e+09   \n",
       "\n",
       "   std_test_neg_mean_squared_error  rank_test_neg_mean_squared_error  \n",
       "4                     2.440747e+09                                 1  \n",
       "5                     2.450602e+09                                 2  \n",
       "0                     2.458551e+09                                 3  \n",
       "1                     2.503412e+09                                 4  \n",
       "3                     2.553022e+09                                 5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the results dataframe by mean_test_r2 in descending order\n",
    "sorted_results_df = results_df.sort_values(by='mean_test_r2', ascending=False)\n",
    "sorted_results_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3712158",
   "metadata": {},
   "source": [
    "To get the overall best model across all the models, we can compare the best scores stored in the best_scores dictionary. Let's provide the code to display the best estimator across all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c3c07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('poly',\n",
       "                                                                   PolynomialFeatures(include_bias=False))]),\n",
       "                                                  ['GrLivArea', 'OverallQual',\n",
       "                                                   'YearBuilt', 'TotalBsmtSF',\n",
       "                                                   'GarageCars', 'GarageArea',\n",
       "                                                   '1stFlrSF', 'FullBath']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['CentralAir',\n",
       "                                                   'ExterQual'])])),\n",
       "                ('regressor', ElasticNet(alpha=0.1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the model with the highest R^2 score\n",
    "best_model_name = max(best_r2_scores, key=best_r2_scores.get)\n",
    "\n",
    "# Display the best estimator across all models\n",
    "best_overall_model = best_models[best_model_name]\n",
    "best_overall_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e63ce",
   "metadata": {},
   "source": [
    "Based on the results from the `GridSearchCV` and the provided hyperparameter grid, the ElasticNet model with an `alpha` value of 0.1 performed the best in terms of the chosen evaluation metric (negative mean squared error) during cross-validation on the training data.\n",
    "\n",
    "Remember, the `alpha` parameter in ElasticNet controls the strength of the regularization. A smaller `alpha` means less regularization and a model that's more likely to fit the training data closely, while a larger `alpha` increases the regularization strength, which can make the model more general and potentially prevent overfitting.\n",
    "\n",
    "In the context of the hyperparameters we tried `([0.01, 0.1, 1, 10, 100])`, an `alpha` of 0.1 is on the smaller end of the spectrum. This means the chosen model has relatively less regularization compared to models with higher `alpha` values from the provided grid. However, it's still regularized more than if `alpha` were set to a very tiny value like 0.01.\n",
    "\n",
    "It's also worth noting that while this model performed the best on the training data during cross-validation, it's essential to evaluate its performance on a separate test set to get a true sense of its predictive capabilities on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6d8f2",
   "metadata": {},
   "source": [
    "## 7. Making Predictions <a id='ch7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe71a8",
   "metadata": {},
   "source": [
    "Having trained various models on the Ames Housing Dataset, our Elastic Net model with an alpha of\n",
    "0.1 emerged as the best performer on the training data. To further assess its efficacy, it's essential to evaluate its predictions on data it hasn't encountered during training. This step is pivotal, offering insights into the model's potential real-world performance and robustness.\n",
    "\n",
    "To make predictions on the testing data, we utilize the `predict` method of our best-performing Elastic Net model. This method ingests the features from our testing set (`X_test`) and outputs the predicted values for our target variable, `SalePrice`.\n",
    "\n",
    "The following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7618a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target values for the testing data using the best model\n",
    "y_pred = best_overall_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cd846",
   "metadata": {},
   "source": [
    "carries out this prediction, saving the results in the `y_pred` variable. Subsequently, we can juxtapose these predicted values with the actual ones to gauge the accuracy and reliability of our chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998bfe1",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Model <a id='ch8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a8d82",
   "metadata": {},
   "source": [
    "After making predictions with our trained model, it's crucial to evaluate its performance. Two common metrics used for regression tasks are:\n",
    "\n",
    "1. <span style=\"color:red\">**Mean Squared Error (MSE)**</span>: Measures the average squared difference between the actual values (`y_test`) and the predicted values (`y_pred`). A lower MSE indicates a better fit of the model to the data, while a higher MSE suggests potential underfitting or overfitting. The formula for MSE is given by:\n",
    "\n",
    "$$\\Large \\displaystyle \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    " - $n$ is the total number of observations\n",
    " - $y_i$ is the actual value of the observation\n",
    " - $\\hat{y}_i$ is the predicted value of the observation\n",
    "\n",
    "2. <span style=\"color:red\">**Coefficient of Determination ($R^2$ value)**</span>: Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An $R^2$ value closer to 1 indicates a better fit. The formula for $R^2$ is:\n",
    "\n",
    "$$\\Large \\displaystyle R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}$$\n",
    "\n",
    " - $\\text{SS}_{\\text{res}}$ represents the the sum of the squared residuals\n",
    " - $\\text{SS}_{\\text{tot}}$ represents the total sum of squares\n",
    "\n",
    "In this section, we compute both the MSE using scikit-learn's `mean_squared_error` function and the $R^2$ value using the `score` method of our trained model. These values give us a quantitative measure of how well our linear regression model predicts house prices based on the living area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "245ac0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 654287636.08\n",
      "R-squared Value: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared Value: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe905dcd",
   "metadata": {},
   "source": [
    "## 9. Conclusion: <a id='ch9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422292c3",
   "metadata": {},
   "source": [
    "Throughout this analysis, we delved deep into the Ames Housing dataset, exploring the relationships between various features and the target variable: house prices. Our journey began with a simple linear regression model, which provided a foundational understanding of the data's structure and the potential predictors.\n",
    "\n",
    "The initial model, focusing solely on the living area of houses, achieved an \\$R^2\\$ value of 49.2%. This was a decent starting point, but it indicated that nearly half of the variability in house prices was left unexplained by the model.\n",
    "\n",
    "As we expanded our horizons, incorporating a broader set of features, the results became increasingly promising. A multiple regression model that included variables like the overall quality of materials and finishes, and the exterior quality, achieved an \\$R^2\\$ value of 82.7%. This was a substantial leap from the simple model, explaining over 82% of the variability in house prices.\n",
    "\n",
    "However, our best model, an Elastic Net model with an alpha of 0.1, surpassed even that, achieving an \\$R^2\\$ value of 85%. This model not only outperformed the simple linear regression but also the multiple regression model, underscoring the power of regularization in enhancing predictive accuracy.\n",
    "\n",
    "Furthermore, the **mean squared error (MSE)**, a metric indicating the average squared difference between actual and predicted prices, saw consistent reductions as we refined our models. This trajectory underscores the enhanced accuracy achieved with each iteration.\n",
    "\n",
    "In conclusion, the size of the living area, the overall quality of materials and finishes, and the exterior quality play pivotal roles in determining house prices in the Ames Housing dataset. All three variables positively influence the price, emphasizing the premium placed on size, quality, and aesthetic appeal in the real estate market. The progression from our simple to the Elastic Net model underscores the importance of feature selection, regularization, and the potential of machine learning in making more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f875c3",
   "metadata": {},
   "source": [
    "### Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa262c0",
   "metadata": {},
   "source": [
    "1. <span style=\"color:red\">**Higher Degree Polynomials**</span>: Introducing higher-degree polynomials can capture more complex relationships in the data. However, care should be taken to avoid overfitting.\n",
    "2. <span style=\"color:red\">**Expanded Hyperparameter Search**</span>: While we explored a range of values for alpha in our Elastic Net model, a more exhaustive search or using techniques like RandomizedSearchCV could potentially find even better hyperparameters.\n",
    "3. <span style=\"color:red\">**Feature Engineering**</span>: Creating new features or transforming existing ones can sometimes unveil hidden patterns in the data.\n",
    "4. <span style=\"color:red\">**Ensemble Methods**</span>: Combining predictions from multiple models can often lead to more robust and accurate predictions.\n",
    "\n",
    "By continually refining our approach and exploring new techniques, we can hope to push the boundaries of predictive accuracy even further in future analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
